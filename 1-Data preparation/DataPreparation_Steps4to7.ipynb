{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef1af052-90ec-4cad-890d-ac17807a41fb",
   "metadata": {},
   "source": [
    "# 1- Implementing the Preparation tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45a70632-066a-4318-a753-578f3264d590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import timedelta\n",
    "from dask.diagnostics import ProgressBar\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import preparation as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aed4ffb7-c534-4e15-913c-feec4159a71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ed4b7c-6013-4ab7-9221-201ec3af6bee",
   "metadata": {},
   "source": [
    "\n",
    "### 1.1- filtered: contains records up to the occurrence of the first outcome or the entire group if no there is no outcome present.\n",
    "### 1.2- filtered_with_events: contains the records with event_id and the records have been expanded, as each record has only one event.\n",
    "\n",
    "The aim of the filtered data frame is to be used for the matrix method \n",
    "\n",
    "The aim of the filtered data frame with the event ID and expanded events is to be used in the sliding windowing method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0eda0307-349d-4f5f-80e4-084b8fff8c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from multiple h5 files...\n",
      "Extracting Records up to the occurrence of the first outcome or the entire group if no there is no outcome presents were filtered\n",
      "Processing the dataset using the 'process_dataset' function...\n",
      "[                                        ] | 0% Completed | 108.03 ms"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fuzzysha/opt/anaconda3/envs/hproj/lib/python3.9/site-packages/dask/dataframe/multi.py:1297: UserWarning: Concatenating dataframes with unknown divisions.\n",
      "We're assuming that the indices of each dataframes are \n",
      " aligned. This assumption is not generally safe.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 72.91 s\n",
      "Stacked Data Shape: (31989618, 68)\n",
      "Number of Partitions: 32\n",
      "Applying the filtering function to the dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fuzzysha/Desktop/PHD_plan/Codes/paper_codes/paper_2_pre_processing_methods/preparation.py:219: UserWarning: `meta` is not specified, inferred from partial data. Please provide `meta` if the result is unexpected.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta={'x': 'f8', 'y': 'f8'}) for dataframe result\n",
      "  or:     .apply(func, meta=('x', 'f8'))            for series result\n",
      "  filtered = grouped_data.apply(filter_records)\n",
      "/Users/fuzzysha/opt/anaconda3/envs/hproj/lib/python3.9/site-packages/dask/dataframe/core.py:6003: UserWarning: \n",
      "You did not provide metadata, so Dask is running your function on a small dataset to guess output types. It is possible that Dask will guess incorrectly.\n",
      "To provide an explicit output types or to silence this message, please provide the `meta=` keyword, as described in the map or apply function that you are using.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta=(None, 'object'))\n",
      "\n",
      "  warnings.warn(meta_warning(meta))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data frame with first outcome was filtered successfully\n",
      "Adding events in the filtered dataset using the 'preprocess_events' function...\n",
      "Event IDs have been added, and the records have been expanded, as each record now contains only one event.\n"
     ]
    }
   ],
   "source": [
    "# Read data from multiple h5 files into a Dask DataFrame\n",
    "# Each file is read in parallel as a separate partition\n",
    "print(\"Reading data from multiple h5 files...\")\n",
    "ddf = dd.read_hdf('data/df_subjects_h5/*.h5', key='df_subjects')\n",
    "\n",
    "# 1-filtering the records up until the first outcome.\n",
    "print('Extracting Records up to the occurrence of the first outcome or the entire group if no there is no outcome presents were filtered')\n",
    "filtered=pp.getRecordsUntilOutcome(ddf)\n",
    "print('data frame with first outcome was filtered successfully')\n",
    "\n",
    "# 2- adding events id and expanding the events\n",
    "\n",
    "# Add events in the filtered dataset using the 'preprocess_events' function\n",
    "print(\"Adding events in the filtered dataset using the 'preprocess_events' function...\")\n",
    "filtered_with_events = pp.preprocess_events(filtered)\n",
    "\n",
    "print('Event IDs have been added, and the records have been expanded, as each record now contains only one event.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67494731-1797-4a7c-b3cd-3fb688332a5d",
   "metadata": {},
   "source": [
    "#### 1.3- Get the unique subject ids and Split them into training_wo_vlidation, training_w_validation, validation, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db31d4ad-c0c5-4273-878c-59ae65c3dc12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting the subjects into train, test, and validation sets and return them as the list...\n",
      "Computing unique subject_ids...\n",
      "Group the subject and Unique subject_ids as a list was computed successfully!\n",
      "The number of individuals in the training set: 216197\n",
      "The number of individuals in training data set with deduction of validation is: 194577\n",
      "The number of individuals in the test data set is: 24022\n",
      "The number of individuals in the validation data set  is: 21620\n",
      " Creating the train, test, and validation based on the subject train, test, and validation for the filtered data frame...\n",
      "Computing train, test, and validation dataframes for : filtered\n",
      "The number of records in the training set: 27581683\n",
      "The number of individuals in the training set: 216197\n",
      "The number of records in the train_data_w_val set: 24805864\n",
      "The number of individuals in the train_data_w_val set: 194577\n",
      "The number of records in the test_data set: 3045651\n",
      "The number of individuals in the test_data set: 24022\n",
      "The number of records in the validation_data set: 2775819\n",
      "The number of individuals in the validation_data set: 21620\n",
      "Saving the filtered and train test and validation Dask DataFrame as a parquet file...\n",
      "Creating the train, test, and validation based on the subject train, test, and validation for the filtered_with_events data frame...\n",
      "Computing train, test, and validation dataframes for : filtered_with_events\n",
      "The number of records in the training set: 27581683\n",
      "The number of individuals in the training set: 216197\n",
      "The number of records in the train_data_w_val set: 24805864\n",
      "The number of individuals in the train_data_w_val set: 194577\n",
      "The number of records in the test_data set: 3045651\n",
      "The number of individuals in the test_data set: 24022\n",
      "The number of records in the validation_data set: 2775819\n",
      "The number of individuals in the validation_data set: 21620\n",
      "Saving the filtered and the splitter Dask DataFrame as a parquet file...\n",
      "finished the jobs successfully\n"
     ]
    }
   ],
   "source": [
    "id ='subject_id'\n",
    "\n",
    "print ('splitting the subjects into train, test, and validation sets and return them as the list...' )\n",
    "train, train_w_val, test, validation=pp.split_subject_ids(ddf,id)\n",
    "##################################################################################################\n",
    "print(' Creating the train, test, and validation based on the subject train, test, and validation for the filtered data frame...')\n",
    "ddf_name='filtered'\n",
    "ddf=filtered\n",
    "train_filtered_data,train_filtered_data_w_val,test_filtered_data,validation_filtered_data=pp.split_datasets(ddf,ddf_name,id,train, train_w_val, test, validation)\n",
    "################################################################################################\n",
    "print ('Saving the filtered and train test and validation Dask DataFrame as a parquet file...')\n",
    "\n",
    "ddf_path=\"data/df_subjects_first_outcome/matrix_pp/\"\n",
    "\n",
    "train_filtered_data.to_parquet(ddf_path+\"train/\", write_index=False)\n",
    "train_filtered_data_w_val.to_parquet(ddf_path+\"train_w_val/\", write_index=False)\n",
    "test_filtered_data.to_parquet(ddf_path+\"test/\", write_index=False)\n",
    "validation_filtered_data.to_parquet(ddf_path+\"validation/\", write_index=False)\n",
    "filtered.to_parquet(ddf_path+\"filtered/\", write_index=False)\n",
    "################################################################################################\n",
    "\n",
    "print ('Creating the train, test, and validation based on the subject train, test, and validation for the filtered_with_events data frame...')\n",
    "\n",
    "ddf_name='filtered_with_events'\n",
    "ddf=filtered_with_events\n",
    "\n",
    "train_filtered_with_events_data,train_filtered_with_events_data_w_val,test_filtered_with_events_data,validation_filtered_with_events_data=pp.split_datasets(ddf,ddf_name,id,train, train_w_val, test, validation)\n",
    "###################################################################################################\n",
    "\n",
    "print ('Saving the filtered and the splitter Dask DataFrame as a parquet file...')\n",
    "\n",
    "ddf_path=\"data/df_subjects_first_outcome/sliding_pp/\"\n",
    "\n",
    "train_filtered_with_events_data.to_parquet(ddf_path+\"train/\", write_index=False)\n",
    "train_filtered_with_events_data_w_val.to_parquet(ddf_path+\"train_w_val/\", write_index=False)\n",
    "test_filtered_with_events_data.to_parquet(ddf_path+\"test/\", write_index=False)\n",
    "validation_filtered_with_events_data.to_parquet(ddf_path+\"validation/\", write_index=False)\n",
    "filtered_with_events.to_parquet(ddf_path+\"filtered_with_events/\", write_index=False)\n",
    "\n",
    "#################################################################################################\n",
    "\n",
    "print(\"finished the jobs successfully\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
